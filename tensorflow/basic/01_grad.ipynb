{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 붓꽃의 품종 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서플로 버전: 2.0.0-rc1\n",
      "즉시 실행: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"텐서플로 버전: {}\".format(tf.__version__))\n",
    "print(\"즉시 실행: {}\".format(tf.executing_eagerly()))\n",
    "np.set_printoptions(precision=3, linewidth=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 측정된 꽃받침과 꽃잎의 길이와 폭을 토대로 붓꽃을 분류하는 모델을 통해 경사하강법(GD) 학습\n",
    "* Iris setosa\n",
    "* Iris virginica\n",
    "* Iris versicolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![꽃](https://www.tensorflow.org/images/iris_three_species.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋이 복사된 위치: /home/woong/.keras/datasets/iris_training.csv\n",
      "120,4,setosa,versicolor,virginica\n",
      "6.4,2.8,5.6,2.2,2\n",
      "5.0,2.3,3.3,1.0,1\n",
      "4.9,2.5,4.5,1.7,2\n",
      "4.9,3.1,1.5,0.1,0\n",
      "30,4,setosa,versicolor,virginica\n",
      "5.9,3.0,4.2,1.5,1\n",
      "6.9,3.1,5.4,2.1,2\n",
      "5.1,3.3,1.7,0.5,0\n",
      "6.0,3.4,4.5,1.6,1\n"
     ]
    }
   ],
   "source": [
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "test__dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url), origin=train_dataset_url)\n",
    "test__dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(test__dataset_url), origin=test__dataset_url)\n",
    "\n",
    "print(\"데이터셋이 복사된 위치: {}\".format(train_dataset_fp))\n",
    "!head -n5 {train_dataset_fp}\n",
    "!head -n5 {test__dataset_fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "레이블: species\n"
     ]
    }
   ],
   "source": [
    "# column_name = ['꽃잎 길이', '꽃잎 너비', '꽃받침 길이', '꽃받침 너비', '종']\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "# species = 0: Iris setosa, 1: Iris versicolor, 2: Iris virginica\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"특성: {}\".format(feature_names))\n",
    "print(\"레이블: {}\".format(label_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    shuffle=False,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "plt.scatter(features['petal_length'], features['sepal_length'], c=labels, cmap='viridis')\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"특성들을 단일 배열로 묶습니다.\"\"\"\n",
    "# tf.stack: Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function pack_features_vector at 0x7f5cf267fa60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function pack_features_vector at 0x7f5cf267fa60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, 4), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.4 2.8 5.6 2.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.7 3.8 1.7 0.3]], shape=(5, 4), dtype=float32) tf.Tensor([2 1 2 0 0], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "print(features[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 (pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           6.4          2.8           5.6          2.2\n",
       "1           5.0          2.3           3.3          1.0\n",
       "2           4.9          2.5           4.5          1.7\n",
       "3           4.9          3.1           1.5          0.1\n",
       "4           5.7          3.8           1.7          0.3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas.read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
    "pdFeature = pd.read_csv(train_dataset_fp, sep=',', skiprows=1, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "# pandas.DataFrame.pop : Return item and drop from frame. Raise KeyError if not found.\n",
    "pdLabel = pdFeature.pop('species')\n",
    "\n",
    "# pandas.DataFrame.head : Return the first n rows.\n",
    "pdFeature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.data.Dataset.from_tensor_slices: Creates a Dataset whose elements are slices of the given tensors..\n",
    "pdDataset = tf.data.Dataset.from_tensor_slices((pdFeature.values, pdLabel.values))\n",
    "pdFeature.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: 0, Features: [6.4 2.8 5.6 2.2], Species: 2\n",
      "Row: 1, Features: [5.  2.3 3.3 1. ], Species: 1\n",
      "Row: 2, Features: [4.9 2.5 4.5 1.7], Species: 2\n",
      "Row: 3, Features: [4.9 3.1 1.5 0.1], Species: 0\n",
      "Row: 4, Features: [5.7 3.8 1.7 0.3], Species: 0\n"
     ]
    }
   ],
   "source": [
    "for row, data in enumerate(pdDataset.take(5)):\n",
    "    print ('Row: {}, Features: {}, Species: {}'.format(row, data[0], data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/images/custom_estimators/full_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.Sequential: Linear stack of layers.\n",
    "# Please check the configuration of each layer (press 'Shift+Tab' in xx.Dense(here!))\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # Weight:  4-inputs * 10-dense =  40,  Bias: 10, Total:  50\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),                    # Weight: 10-inputs * 10-dense = 100,  Bias: 10, Total: 110\n",
    "  tf.keras.layers.Dense(3)                                             # Weight: 10-inputs *  3-dense =  30,  Bias:  3, Total:  33\n",
    "])\n",
    "model.summary()\n",
    "#help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 (Training) 전 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (32, 4)\n",
      "Layer 0: Weight(4, 10) Bias(10,) Activation(None, 10)\n",
      "Layer 1: Weight(10, 10) Bias(10,) Activation(None, 10)\n",
      "Layer 2: Weight(10, 3) Bias(3,) Activation(None, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Input Shape: {}'.format(features.shape))\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer {}: Weight{} Bias{} Activation{}'.format(i, model.layers[i].weights[0].shape, model.layers[i].bias.shape, model.layers[i].output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 1st Features:\n",
      " [6.4 2.8 5.6 2.2]\n",
      "\n",
      ">> Weights (Input-Layer1):\n",
      " [[-0.167 -0.257 -0.068 -0.527 -0.582 -0.623 -0.203 -0.606 -0.289 -0.349]\n",
      " [-0.337 -0.003 -0.54  -0.258  0.235 -0.642 -0.234 -0.634  0.057 -0.245]\n",
      " [-0.301 -0.376  0.417  0.151 -0.446 -0.126  0.287  0.495  0.077  0.385]\n",
      " [ 0.595 -0.653 -0.183 -0.379  0.155  0.447  0.41  -0.546 -0.479 -0.411]]\n",
      "\n",
      ">> Biases   (Input-Layer1):\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      ">> Layer1 MatMul[0,0]: -2.3855338\n",
      "\n",
      ">> Layer1 MatMul: \n",
      " [[-2.386 -5.193 -0.014 -4.08  -5.225 -5.505  0.557 -4.084 -2.312 -1.669]\n",
      " [-2.006 -3.184 -0.389 -3.107 -3.688 -4.559 -0.195 -3.402 -1.539 -1.45 ]\n",
      " [-2.001 -4.067 -0.117 -3.19  -4.009 -4.463  0.41  -3.256 -1.741 -1.29 ]\n",
      " [-2.252 -1.897 -1.4   -3.192 -2.779 -5.185 -1.247 -4.248 -1.172 -1.934]\n",
      " [-2.562 -2.311 -1.786 -3.84  -3.138 -6.069 -1.434 -5.188 -1.443 -2.39 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('>> 1st Features:\\n %s\\n'%(features[0].numpy()))\n",
    "print('>> Weights (Input-Layer1):\\n %s\\n'%(model.layers[0].weights[0].numpy()))\n",
    "print('>> Biases   (Input-Layer1):\\n %s\\n'%(model.layers[0].bias.numpy()))\n",
    "print('>> Layer1 MatMul[0,0]: %s\\n'%(np.sum(features[0]*model.layers[0].weights[0][:,0])))\n",
    "print('>> Layer1 MatMul: \\n %s\\n'%(tf.matmul(features, model.layers[0].weights[0]).numpy()[:5]))\n",
    "\n",
    "actLayer0 = tf.nn.relu(tf.matmul(features,  model.layers[0].weights[0]) + tf.ones([batch_size, 1]) * model.layers[0].bias)\n",
    "actLayer1 = tf.nn.relu(tf.matmul(actLayer0, model.layers[1].weights[0]) + tf.ones([batch_size, 1]) * model.layers[1].bias)\n",
    "actLayer2 =            tf.matmul(actLayer1, model.layers[2].weights[0]) + tf.ones([batch_size, 1]) * model.layers[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=288, shape=(5, 3), dtype=float32, numpy=\n",
       "array([[ 0.298, -0.165,  0.08 ],\n",
       "       [ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.22 , -0.122,  0.059],\n",
       "       [ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(features)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=292, shape=(5, 3), dtype=float32, numpy=\n",
       "array([[ 0.298, -0.165,  0.08 ],\n",
       "       [ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.22 , -0.122,  0.059],\n",
       "       [ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actLayer2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=306, shape=(5, 10), dtype=float32, numpy=\n",
       "array([[0.044, 0.   , 0.   , 0.251, 0.204, 0.   , 0.   , 0.042, 0.139, 0.227],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.032, 0.   , 0.   , 0.185, 0.15 , 0.   , 0.   , 0.031, 0.103, 0.167],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1](model.layers[0](features))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=310, shape=(5, 10), dtype=float32, numpy=\n",
       "array([[0.044, 0.   , 0.   , 0.251, 0.204, 0.   , 0.   , 0.042, 0.139, 0.227],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.032, 0.   , 0.   , 0.185, 0.15 , 0.   , 0.   , 0.031, 0.103, 0.167],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actLayer1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 적용 (로짓(logit)을 각 클래스에 대한 확률로 변환)\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/bdc1f8eaa8064d15893f1ba6426f20ff8e7149c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Prob. per Class: [0.411 0.259 0.33 ]\n",
      "Index: 1, Prob. per Class: [0.333 0.333 0.333]\n",
      "Index: 2, Prob. per Class: [0.39  0.277 0.332]\n",
      "Index: 3, Prob. per Class: [0.333 0.333 0.333]\n",
      "Index: 4, Prob. per Class: [0.333 0.333 0.333]\n"
     ]
    }
   ],
   "source": [
    "# Using Numpy\n",
    "for idx, logitRow in enumerate(predictions[:5]):\n",
    "    print('Index: {}, Prob. per Class: {}'.format(idx, np.exp(logitRow)/np.sum(np.exp(logitRow))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=339, shape=(5, 3), dtype=float32, numpy=\n",
       "array([[0.411, 0.259, 0.33 ],\n",
       "       [0.333, 0.333, 0.333],\n",
       "       [0.39 , 0.277, 0.332],\n",
       "       [0.333, 0.333, 0.333],\n",
       "       [0.333, 0.333, 0.333]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TensorFlow\n",
    "tf.nn.softmax(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  예측: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "레이블: [2 1 2 0 0 0 0 2 1 0 1 1 0 0 2 1 2 2 2 0 2 2 0 2 2 0 1 2 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"  예측: {}\".format(tf.argmax(predictions, axis=1)))\n",
    "print(\"레이블: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수 (Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차 (Mean Squared Error, MSE) : Regression (회귀) 문제에 주로 사용\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교차 엔트로피 오차 (Cross Entropy Error, CEE) : Classification (분류) 문제에 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c6b895514e10a3ce88773852cba1cb1e248ed763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1027418933808804\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Error Using Numpy\n",
    "cee = 0\n",
    "for idx, prob in enumerate(tf.nn.softmax(model(features))):\n",
    "    y  = labels[idx]\n",
    "    y_ = prob[y]\n",
    "    cee = cee - np.log(y_)/batch_size\n",
    "print(cee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.102742\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.losses.SparseCategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
    "lossCEE = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "print(lossCEE(y_true=labels, y_pred=model(features)).numpy())\n",
    "def loss(model, x, y):\n",
    "  y_ = model(x)\n",
    "  return lossCEE(y_true=y, y_pred=y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cs231n.github.io/assets/nn3/opt1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.GradientTape: Record operations for automatic differentiation.\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "# tf.keras.optimizers: Built-in optimizer classes.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "var = optimizer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단계: 5, 초기 손실: 0.9541771411895752\n",
      "단계: 6,      손실: 0.938339352607727\n"
     ]
    }
   ],
   "source": [
    "loss_value, grads = grad(model, features, labels)\n",
    "\n",
    "print(\"단계: {}, 초기 손실: {}\".format(optimizer.iterations.numpy(), loss_value.numpy()))\n",
    "weights_pre  = optimizer.weights[1].numpy()\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "print(\"단계: {},      손실: {}\".format(optimizer.iterations.numpy(), loss(model, features, labels).numpy()))\n",
    "weights_post = optimizer.weights[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.052,  0.   ,  0.   ,  0.   , -0.005,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.023,  0.   ,  0.   ,  0.   , -0.001,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.003,  0.   ,  0.   ,  0.   , -0.023,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   , -0.007,  0.   ,  0.   ,  0.   , -0.014,  0.   ,  0.   ,  0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.052,  0.   ,  0.   ,  0.   ,  0.005,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.023,  0.   ,  0.   ,  0.   ,  0.004,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   , -0.004,  0.   ,  0.   ,  0.   , -0.021,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   , -0.011,  0.   ,  0.   ,  0.   , -0.014,  0.   ,  0.   ,  0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3045, shape=(4, 10), dtype=float32, numpy=\n",
       "array([[ 0.   ,  0.   ,  0.047,  0.   ,  0.   ,  0.   ,  0.098,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.022,  0.   ,  0.   ,  0.   ,  0.049,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   , -0.063,  0.   ,  0.   ,  0.   ,  0.002,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   , -0.045,  0.   ,  0.   ,  0.   , -0.02 ,  0.   ,  0.   ,  0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\n",
    "$$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\n",
    "$$t := 0 \\text{(Initialize timestep)}$$\n",
    " |      \n",
    " |        The update rule for `variable` with gradient `g` uses an optimization\n",
    " |        described at the end of section 2 of the paper:\n",
    " |      \n",
    " |        $$t := t + 1$$\n",
    " |        $$lr_t := \\text{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
    " |      \n",
    " |        $$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
    " |        $$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
    " |        $$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Adam in module tensorflow.python.keras.optimizer_v2.adam object:\n",
      "\n",
      "class Adam(tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2)\n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  Adam optimization is a stochastic gradient descent method that is based on\n",
      " |  adaptive estimation of first-order and second-order moments.\n",
      " |  According to the paper\n",
      " |  [Adam: A Method for Stochastic Optimization. Kingma et al.,\n",
      " |  2014](http://arxiv.org/abs/1412.6980),\n",
      " |   the method is \"*computationally efficient, has little memory\n",
      " |  requirement, invariant to diagonal rescaling of gradients, and is well suited\n",
      " |  for problems that are large in terms of data/parameters*\".\n",
      " |  \n",
      " |  For AMSGrad see [On The Convergence Of Adam And Beyond.\n",
      " |  Reddi et al., 5-8](https://openreview.net/pdf?id=ryQu7f-RZ).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Adam\n",
      " |      tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam', **kwargs)\n",
      " |      Construct a new Adam optimizer.\n",
      " |      \n",
      " |      If amsgrad = False:\n",
      " |        Initialization:\n",
      " |      \n",
      " |        $$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\n",
      " |        $$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\n",
      " |        $$t := 0 \\text{(Initialize timestep)}$$\n",
      " |      \n",
      " |        The update rule for `variable` with gradient `g` uses an optimization\n",
      " |        described at the end of section 2 of the paper:\n",
      " |      \n",
      " |        $$t := t + 1$$\n",
      " |        $$lr_t := \\text{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
      " |      \n",
      " |        $$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
      " |        $$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
      " |        $$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
      " |      \n",
      " |      If amsgrad = True:\n",
      " |        Initialization:\n",
      " |      \n",
      " |        $$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\n",
      " |        $$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\n",
      " |        $$v_hat_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\n",
      " |        $$t := 0 \\text{(Initialize timestep)}$$\n",
      " |      \n",
      " |        The update rule for `variable` with gradient `g` uses an optimization\n",
      " |        described at the end of section 2 of the paper:\n",
      " |      \n",
      " |        $$t := t + 1$$\n",
      " |        $$lr_t := \\text{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
      " |      \n",
      " |        $$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
      " |        $$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
      " |        $$v_hat_t := max(v_hat_{t-1}, v_t)$$\n",
      " |        $$variable := variable - lr_t * m_t / (\\sqrt{v_hat_t} + \\epsilon)$$\n",
      " |      \n",
      " |      The default value of 1e-7 for epsilon might not be a good default in\n",
      " |      general. For example, when training an Inception network on ImageNet a\n",
      " |      current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
      " |      formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
      " |      the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
      " |      hat\" in the paper.\n",
      " |      \n",
      " |      The sparse implementation of this algorithm (used when the gradient is an\n",
      " |      IndexedSlices object, typically because of `tf.gather` or an embedding\n",
      " |      lookup in the forward pass) does apply momentum to variable slices even if\n",
      " |      they were not used in the forward pass (meaning they have a gradient equal\n",
      " |      to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
      " |      accumulator. This means that the sparse behavior is equivalent to the dense\n",
      " |      behavior (in contrast to some momentum implementations which ignore momentum\n",
      " |      unless a variable slice was actually used).\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      " |        beta_1: A float value or a constant float tensor. The exponential decay\n",
      " |          rate for the 1st moment estimates.\n",
      " |        beta_2: A float value or a constant float tensor. The exponential decay\n",
      " |          rate for the 2nd moment estimates.\n",
      " |        epsilon: A small constant for numerical stability. This epsilon is\n",
      " |          \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |          Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
      " |        amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n",
      " |          the paper \"On the Convergence of Adam and beyond\".\n",
      " |        name: Optional name for the operations created when applying gradients.\n",
      " |          Defaults to \"Adam\".  @compatibility(eager) When eager execution is\n",
      " |          enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n",
      " |          a callable that takes no arguments and returns the actual value to use.\n",
      " |          This can be useful for changing these values across different\n",
      " |          invocations of optimizer functions. @end_compatibility\n",
      " |        **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
      " |          `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
      " |          gradients by value, `decay` is included for backward compatibility to\n",
      " |          allow time inverse decay of learning rate. `lr` is included for backward\n",
      " |          compatibility, recommended to use `learning_rate` instead.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the optimimizer.\n",
      " |      \n",
      " |      An optimizer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of an optimizer.\n",
      " |      The same optimizer can be reinstantiated later\n",
      " |      (without any saved state) from this configuration.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  __getattribute__(self, name)\n",
      " |      Overridden to support hyperparameter access.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Override setattr to support dynamic hyperparameter setting.\n",
      " |  \n",
      " |  add_slot(self, var, slot_name, initializer='zeros')\n",
      " |      Add a new slot variable for `var`.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer='zeros', trainable=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>)\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, name=None)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs.\n",
      " |        name: Optional name for the returned operation.  Default to the name\n",
      " |          passed to the `Optimizer` constructor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. If `global_step`\n",
      " |        was not None, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |  \n",
      " |  get_gradients(self, loss, params)\n",
      " |      Returns gradients of `loss` with respect to `params`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        loss: Loss tensor.\n",
      " |        params: List of variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of gradient tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
      " |          function not implemented).\n",
      " |  \n",
      " |  get_slot(self, var, slot_name)\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      A list of names for this optimizer's slots.\n",
      " |  \n",
      " |  get_updates(self, loss, params)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |  \n",
      " |  minimize(self, loss, var_list, grad_loss=None, name=None)\n",
      " |      Minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A callable taking no arguments which returns the value to minimize.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable` objects.\n",
      " |          Use callable when the variable list would otherwise be incomplete before\n",
      " |          `minimize` since the variables are created at the first time `loss` is\n",
      " |          called.\n",
      " |        grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      " |        name: Optional name for the returned operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      " |        was not `None`, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |  \n",
      " |  variables(self)\n",
      " |      Returns variables of this Optimizer based on the order created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from abc.ABCMeta\n",
      " |      Creates an optimizer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same optimizer from the config\n",
      " |      dictionary.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the output of get_config.\n",
      " |          custom_objects: A Python dictionary mapping names to additional Python\n",
      " |            objects used to create this optimizer, such as a function used for a\n",
      " |            hyperparameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An optimizer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
      " |  \n",
      " |  iterations\n",
      " |      Variable. The number of training steps this Optimizer has run.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns variables of this Optimizer based on the order created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
