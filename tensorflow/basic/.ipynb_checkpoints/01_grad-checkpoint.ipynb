{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 붓꽃의 품종 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"텐서플로 버전: {}\".format(tf.__version__))\n",
    "print(\"즉시 실행: {}\".format(tf.executing_eagerly()))\n",
    "np.set_printoptions(precision=3, linewidth=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 측정된 꽃받침과 꽃잎의 길이와 폭을 토대로 붓꽃을 분류하는 모델을 통해 경사하강법(GD) 학습\n",
    "* Iris setosa\n",
    "* Iris virginica\n",
    "* Iris versicolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![꽃](https://www.tensorflow.org/images/iris_three_species.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "test__dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url), origin=train_dataset_url)\n",
    "test__dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(test__dataset_url), origin=test__dataset_url)\n",
    "\n",
    "print(\"데이터셋이 복사된 위치: {}\".format(train_dataset_fp))\n",
    "!head -n5 {train_dataset_fp}\n",
    "!head -n5 {test__dataset_fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_name = ['꽃잎 길이', '꽃잎 너비', '꽃받침 길이', '꽃받침 너비', '종']\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "# species = 0: Iris setosa, 1: Iris versicolor, 2: Iris virginica\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"특성: {}\".format(feature_names))\n",
    "print(\"레이블: {}\".format(label_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    shuffle=False,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "plt.scatter(features['petal_length'], features['sepal_length'], c=labels, cmap='viridis')\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"특성들을 단일 배열로 묶습니다.\"\"\"\n",
    "# tf.stack: Stacks a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "print(features[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 (pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
    "pdFeature = pd.read_csv(train_dataset_fp, sep=',', skiprows=1, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "# pandas.DataFrame.pop : Return item and drop from frame. Raise KeyError if not found.\n",
    "pdLabel = pdFeature.pop('species')\n",
    "\n",
    "# pandas.DataFrame.head : Return the first n rows.\n",
    "pdFeature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset.from_tensor_slices: Creates a Dataset whose elements are slices of the given tensors..\n",
    "pdDataset = tf.data.Dataset.from_tensor_slices((pdFeature.values, pdLabel.values))\n",
    "pdFeature.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row, data in enumerate(pdDataset.take(5)):\n",
    "    print ('Row: {}, Features: {}, Species: {}'.format(row, data[0], data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/images/custom_estimators/full_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.Sequential: Linear stack of layers.\n",
    "# Please check the configuration of each layer (press 'Shift+Tab' in xx.Dense(here!))\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # Weight:  4-inputs * 10-dense =  40,  Bias: 10, Total:  50\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),                    # Weight: 10-inputs * 10-dense = 100,  Bias: 10, Total: 110\n",
    "  tf.keras.layers.Dense(3)                                             # Weight: 10-inputs *  3-dense =  30,  Bias:  3, Total:  33\n",
    "])\n",
    "model.summary()\n",
    "#help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 (Training) 전 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input Shape: {}'.format(features.shape))\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer {}: Weight{} Bias{} Activation{}'.format(i, model.layers[i].weights[0].shape, model.layers[i].bias.shape, model.layers[i].output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>> 1st Features:\\n %s\\n'%(features[0].numpy()))\n",
    "print('>> Weights (Input-Layer1):\\n %s\\n'%(model.layers[0].weights[0].numpy()))\n",
    "print('>> Biases   (Input-Layer1):\\n %s\\n'%(model.layers[0].bias.numpy()))\n",
    "print('>> Layer1 MatMul[0,0]: %s\\n'%(np.sum(features[0]*model.layers[0].weights[0][:,0])))\n",
    "print('>> Layer1 MatMul: \\n %s\\n'%(tf.matmul(features, model.layers[0].weights[0]).numpy()[:5]))\n",
    "\n",
    "actLayer0 = tf.nn.relu(tf.matmul(features,  model.layers[0].weights[0]) + tf.ones([batch_size, 1]) * model.layers[0].bias)\n",
    "actLayer1 = tf.nn.relu(tf.matmul(actLayer0, model.layers[1].weights[0]) + tf.ones([batch_size, 1]) * model.layers[1].bias)\n",
    "actLayer2 =            tf.matmul(actLayer1, model.layers[2].weights[0]) + tf.ones([batch_size, 1]) * model.layers[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(features)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actLayer2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1](model.layers[0](features))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actLayer1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 적용 (로짓(logit)을 각 클래스에 대한 확률로 변환)\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/bdc1f8eaa8064d15893f1ba6426f20ff8e7149c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Numpy\n",
    "for idx, logitRow in enumerate(predictions[:5]):\n",
    "    print('Index: {}, Prob. per Class: {}'.format(idx, np.exp(logitRow)/np.sum(np.exp(logitRow))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorFlow\n",
    "tf.nn.softmax(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  예측: {}\".format(tf.argmax(predictions, axis=1)))\n",
    "print(\"레이블: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수 (Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차 (Mean Squared Error, MSE) : Regression (회귀) 문제에 주로 사용\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교차 엔트로피 오차 (Cross Entropy Error, CEE) : Classification (분류) 문제에 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c6b895514e10a3ce88773852cba1cb1e248ed763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Error Using Numpy\n",
    "cee = 0\n",
    "for idx, prob in enumerate(tf.nn.softmax(model(features))):\n",
    "    y  = labels[idx]\n",
    "    y_ = prob[y]\n",
    "    cee = cee - np.log(y_)/batch_size\n",
    "print(cee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.losses.SparseCategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
    "lossCEE = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "print(lossCEE(y_true=labels, y_pred=model(features)).numpy())\n",
    "def loss(model, x, y):\n",
    "  y_ = model(x)\n",
    "  return lossCEE(y_true=y, y_pred=y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cs231n.github.io/assets/nn3/opt1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.GradientTape: Record operations for automatic differentiation.\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "# tf.keras.optimizers: Built-in optimizer classes.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "if optimizer.iterations.numpy() == 0:\n",
    "    m1 = 0\n",
    "    m2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value, grads = grad(model, features, labels)\n",
    "\n",
    "print(\"단계: {}, 초기 손실: {}\".format(optimizer.iterations.numpy(), loss_value.numpy()))\n",
    "weights_pre  = model.layers[0].weights[0].numpy()\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "print(\"단계: {},     손실: {}\\n\".format(optimizer.iterations.numpy(), loss(model, features, labels).numpy()))\n",
    "weights_post = model.layers[0].weights[0].numpy()\n",
    "\n",
    "print(weights_pre,  '\\n\\n')\n",
    "print(weights_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\n",
    "$$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\n",
    "$$t := 0 \\text{(Initialize timestep)}$$\n",
    "$$t := t + 1$$\n",
    "$$lr_t := \\text{learning\\_rate} * \\sqrt{1 - beta_2^t} / (1 - beta_1^t)$$\n",
    "$$m_t := beta_1 * m_{t-1} + (1 - beta_1) * g$$\n",
    "$$v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g$$\n",
    "$$variable := variable - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer.iterations.numpy() != 0:\n",
    "    t   = optimizer.iterations.numpy()\n",
    "    print(t)\n",
    "    lr  = optimizer.learning_rate * tf.sqrt(1-(optimizer.beta_2 ** t)) / (1-(optimizer.beta_1**t))\n",
    "    m1 = optimizer.beta_1.numpy() * m1 + (1-optimizer.beta_1.numpy()) * grads[0].numpy()\n",
    "    m2 = optimizer.beta_2.numpy() * m2 + (1-optimizer.beta_2.numpy()) * grads[0].numpy() ** 2  \n",
    "    \n",
    "weights_cal = weights_pre - lr * m1 / (np.sqrt(m2) + optimizer.epsilon)\n",
    "print(weights_cal)\n",
    "print(weights_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 노트: 이 셀을 다시 실행하면 동일한 모델의 변수가 사용됩니다.\n",
    "\n",
    "# 도식화를 위해 결과를 저장합니다.\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # 훈련 루프 - 32개의 배치를 사용합니다.\n",
    "    for x, y in train_dataset:\n",
    "        # 모델을 최적화합니다.\n",
    "        loss_value, grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # 진행 상황을 추적합니다.\n",
    "        epoch_loss_avg(loss_value)  # 현재 배치 손실을 추가합니다.\n",
    "        # 예측된 레이블과 실제 레이블 비교합니다.\n",
    "        epoch_accuracy(y, model(x))\n",
    "\n",
    "    # epoch 종료\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"에포크 {:03d}: 손실: {:.3f}, 정확도: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Procedure')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    test__dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name='species',\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataset = test_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "  logits = model(x)\n",
    "  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "  test_accuracy(prediction, y)\n",
    "\n",
    "print(\"테스트 세트 정확도: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([y,prediction],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
